\documentclass{article}

\usepackage{mathtools}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\usepackage[utf8]{inputenc}
\usepackage{pgfplots}

\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\usepackage{enumerate}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}
\newtheorem{thm}{Theorem}[section]
\newtheorem*{thmt*}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{prop}[thm]{Proposition}
\pgfplotsset{compat=1.15}

\newtheorem{defn}{Definition}[section]
\title{Auditing for Multi-Differential Fairness of Black Box Classifiers}
\author{}
\date{January 2019}

\begin{document}
\maketitle

\section{Multi-Differential Fairness}

\subsection{Preliminary}

\paragraph{Notations}
An individual $i$ is defined by a tuple $((x_{i}, a_{i}), y_{i})$, where $x_{i}\in \mathfrak{X}$ denotes individual $i$'s audited features; $a_{i}\in\mathfrak{A}$ denotes her protected attribute; and $y_{i}\in \{-1, 1\}$ is the classification provided by a black box classifier $f$. The auditor draw samples $
\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$ of size $m$ from a distribution $D$ on $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$. 

\bigskip
Features in $\mathfrak{X}$ are not necessarily the ones used to train $f$. First, the auditor may not have access to all features used to train $f$. Secondly, the auditor may decide to leave deliberately some features used to train $f$ out of $\mathfrak{X}$ because she believes that those features should not be used to define similarity among individuals. For example, if $f$ classifies loan according to their probability of repayment, the auditor may consider that credit score should be used to define individual similarity, but that zipcode, because correlated with races, should not be an auditing feature, although it was used to train $f$.  

\paragraph{Assumptions}
In our analysis we make the following assumption:
\begin{assumption}
\label{ass: 1}
For all $x\in \mathfrak{X}$, $Pr[A|X=x] > 0.$
\end{assumption}
Assumption \ref{ass: 1} guarantees that the distribution of auditing features conditional on protected attributes have common support: there is no $x\in \mathfrak{X}$ that reveals perfectly the individual's protected attribute.  

\subsection{Individual Differential Fairness}
\paragraph{Individual Differential Fairness} 
We define differential fairness as the guarantee that that a classifier leaks with negligible probability the protected attribute of an individual.

\begin{defn}(Individual Differential Fairness)
\label{def: idf}
For $\delta \in [0, 1)$, a classifier $f$ is $\ \delta-$ differential fair if for all $x\in\mathfrak{X}$ and all $a\neq a^{'}\in \mathfrak{A}$
\begin{equation}
\label{eq: idf}
    \frac{Pr[A=a|x, y]}{Pr[A=a^{'}|x, y]} \leq e^{\delta}\frac{Pr[A=a|x]}{Pr[A=a^{'}|x]}.
\end{equation}
\end{defn}

The parameter $\delta$ controls the amount of information leaked by $f$ on the distribution of protected attributes of an individual with auditing feature $x$: larger value of $\delta$ implies larger leakage. A fair classifier does not reveal much information about $A$ that is not already revealed by the auditing features $x$. The advantage of individual differential fairness is to distinguish the unfairness caused by $f$  from the one embedded already in the data. Auditing features might strongly correlate with protected attributes (e.g. zipcodes densely populated by a minority) because the data reflects social, cultural and historical biases. Individual differential fairness guarantees that the classifier $f$ does not exacerbate those biases.  

\paragraph{Relation with differential privacy}
There is an analogy between individual differential fairness for classifiers and differential privacy for database queries. Differential privacy as in  \cite{dwork2014algorithmic} guarantees that outcomes from a query is nearly identical when computed on two adjacent databases that differs only by one record. Rewriting the fairness condition \eqref{eq: idf} as 
\begin{equation}
    Pr[Y|a, x] \leq e^{\delta}Pr[Y|a^{'}, x]
\end{equation}
shows that individual differential fairness guarantees that outcomes from a classifier are nearly identical for individuals that differ only by their protected attributes. Individual differential fairness is a privacy guarantee for protected attributes.

\paragraph{Individual Fairness}
The definition \ref{def: idf} is an individual level definition of fairness. Compared to the notion of individual fairness in \cite{dwork2012fairness}, individual differential fairness does not require to explicit a similarity metric. 

\subsection{Multi-differential fairness}
Although useful, the notion of individual differential fairness suffers from one limitation: it cannot be computationally efficiently audited for. Looking for violations of individual differential fairness will require searching over a set of $2^{|\mathfrak{X}|}$ individuals. Moreover, if $\mathfrak{X}$ is rich enough empirically, a sample from a distribution over $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$ has a negligible probability to have two individuals with the same auditing feature $x$ and different protected attributes $a$. 

\bigskip
Therefore, we relax the definition of individual differential fairness and impose differential individual fairness for group of individuals or sub-populations. Formally, $\mathfrak{C}$ denotes a collection of subsets $G$ of $\mathfrak{X}$. The collection $\mathfrak{C}_{\alpha}$ is $\alpha$-strong of for $S\in \mathfrak{C}$, $Pr[x\in S] \geq \alpha$.  

\begin{defn}(Multi-Differential Fairness)
\label{def: mdf}
Consider a $\alpha$-strong collection $\mathbb{C}_{\alpha}$ of sub-populations of $\mathfrak{X}$. For $0\leq \delta$, a classifier $f$ is $(\mathbb{C}_{\alpha}, \delta)$-multi differential fair with respect to $\mathfrak{A}$ if for all protected attributes $a, a^{'}\in \mathfrak{A}$ and for all $S\in \mathbb{C}_{\alpha}$:
\begin{equation}
\label{eq: mdf}
    \frac{Pr[A=a|S, y]}{Pr[A=a^{'}|S, y]} \leq e^{\delta}\frac{Pr[A=a|S]}{Pr[A=a^{'}|S]}.
\end{equation}
\end{defn}

Multi-differential fairness guarantees that the outcome of a classifier $f$ is nearly mean-independent of protected attributes within any sub-population $G\in \mathfrak{C}$: for all protected attributes $a\neq a^{'}\in \mathfrak{A}$, $Pr[Y|A=a, S] \leq e^{\delta} Pr[Y|A=a^{'}, S]$. The parameter $\delta$ controls for the amount of information related to protected attributes that the classifier leaks: smaller value of $\delta$ means smaller leakage. 


\paragraph{Collection of Indicators.}
the collection of sub-population $\mathbb{C}$ can be equivalently thought as a family of indicators: for each $S\in \mathbb{C}$, there is an indicator $c_{S}: \mathfrak{X}\rightarrow \{-1, 1\}$ such that $c_{S}(x)=1$ if and only if $x\in S$. The relaxation of differential fairness to a collection of groups or sub-population is akin to \cite{kim2018fairness}, \cite{kearns2017preventing} or \cite{hebert2017calibration} where $\mathfrak{C}$ is the computational bound on the granularity of their definition of fairness. The richer $\mathbb{C}$, the stronger the fairness guarantee offers by definition \ref{def: mdf}. However, the complexity of $\mathbb{C}$ is limited by the fact that we identify a sub-population $S$ via random samples drawn from a distribution over $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$. The rest of this paper shows that auditing for multi-differential fairness in polynomial time requires to limit the complexity of $\mathbb{C}$. Potential candidates for $\mathbb{C}$ will be the family short-decision trees or the set of conjunctions of constant number of boolean features. Therefore, auditing for multi-differential fairness will not check whether the fairness condition \eqref{eq: mdf} holds for \textit{all} sub-populations of $\mathfrak{X}$, but only check  the fairness condition for all sub-populations that can be \textit{efficiently identifiable}. 

\section{Auditing, Agnostic Learning and PAC learning}

The definition of multi-differential fairness requires to verify that in no sub-population $S\in \mathbb{C}$, the classifier leaks information about the distribution of protected attributes. If $\mathbb{C}$ is a rich and large class of subset of the feature space $\mathfrak{X}$, an auditing algorithm linearly dependent on $|\mathbb{C}|$ can be prohibitively expensive. In this section we show that finding an auditing algorithm reduces to agnostic learning of the class of sub-populations $\mathbb{C}$. That is, there is no $\log(\mathbb{C})$ running time auditing algorithm unless $\mathbb{C}$ is efficiently agnostically learnable.  

\subsection{Certifying Fairness and Agnostic Learning}
Auditing for multi-differential fairness consists firstly, in establishing there exists a fairness violation; secondly, in identifying a sub-population $S$ that violates the most the fairness condition in \ref{def: mdf}. 

\paragraph{Balanced Distribution}
Suppose that for any $a \in \mathfrak{A}$, we have oracle access to the importance sampling weight $w_{a}(x)=\frac{1 - P[A=a|x]}{P[A=a|x]}$. Therefore, from any distribution $D_{f}$ over $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$ a balanced distribution $D_{f}^{w}$ with $Pr_{w}[A|x]= 1 - Pr_{w}[A|x]$ can be obtained. We first explore certifying fairness for balanced distribution. With a balanced distribution, the multi differential fairness condition can be rewritten as follows: for all protected attributes $a\in \mathfrak{A}$ and for all $S\in \mathbb{C}_{\alpha}$

\begin{equation}
    \label{eq: mdf_w}
    Pr_{w}[A=a |S, y] \leq \frac{e^{\delta}}{e^{\delta} + 1},
\end{equation}
where the sub-script $w$ reminds that the distribution $D_{f}^{w}$ is balanced. Since the distribution $D_{f}^{w}$ induced by $f$ is balanced, auditing features $x$ do not reveal any information on protected attributes and multi-differential fairness can then be interpreted as an upper bound on ability to predict $A$ given the classifier's outcome for any sub-population $S\in \mathbb{C}_{\alpha}$. A $\gamma$-unfairness certificate is a subset $S\in \mathbb{C}$ such that there exists $y\in \{-1, 1\}$ with
\begin{equation}
    Pr_{w}[S, y]Pr_{w}[A=a |S, y] \geq \gamma.
\end{equation}
If there exists a $\gamma$-unfairness certificate, $f$ is $\gamma$-multi differential unfair; otherwise, $f$ is $\gamma$-multi differential fair. Therefore, with balanced distribution, certifying multi-differential fairness is akin to searching for $\gamma$-unfairness certificate. 

\begin{defn}(Certifying Multi-Differential Fairness). 
Let $\gamma, \epsilon >0$, $\eta\in (0, 1)$ and $\mathbb{C}_{\alpha}$ be an $\alpha$-strong collection of sub-populations in $\mathfrak{X}$. An $(\epsilon, \eta)$- certifying algorithm $M(\epsilon, \delta)$ is an algorithm that for any sample from a distribution $D_{f}$ induced by a classifier over $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$, outputs a $\gamma-\epsilon$-unfairness certificate with probability $1-\eta$ whenever $f$ is $ \gamma$-multi differential unfair; and, certifies fairness with probability $1-\eta$ whenever $f$ is is $\gamma$-multi differential fair. 

\bigskip
Moreover, $M(\epsilon, \delta)$ is an efficient certifying algorithm if it requires $poly(\log(|\mathfrak{C}_{\alpha}|, \log(1/\eta), 1/\epsilon))$ samples and runs in $poly(\log(|\mathfrak{C}_{\alpha}|, \log(1/\eta), 1/\epsilon))$. 
\end{defn}

Searching for $\gamma$-unfairness certificate can be formulated as a problem of detecting correlations. For $g, h:\mathfrak{X}\rightarrow \{-1, 1\}$,   let $\langle g, h\rangle_{D}\equiv E_{D}g(x)h(x)$ denote the average inner product between $g$ and $h$. Then, $S$ is a $\gamma$-unfair certificate of $f$ if and only if
\begin{equation}
    \max\left\{\langle \frac{1+c_{S}}{2}, \mathbbm{1}_{a}\frac{y + 1}{2}\rangle_{D_{w}^{f}}, \langle \frac{1+c_{S}}{2}, \mathbbm{1}_{a}\frac{1- y}{2}\rangle_{D_{w}^{f}}\right\} \geq \gamma,
\end{equation}
where $\mathbbm{1}_{a}(x, a^{'})$ is an indicator for protected attribute $a$. Detecting correlation can be phrased as an agnostic learning problem. A concept class $\mathbb{C}$ is agnostically efficiently learnable if and only if there exists an algorithm $\mathfrak{M}$ that given access to a distribution $\{x_{i}, g(x_{i})\}\sim D\times \{-1, 1\}$ and for all $\epsilon, \eta >0$ in $poly(\log(|\mathfrak{C}, \log(\frac{1}{\delta}), \epsilon)$ outputs with probability $1-\eta$ a function $h\in \mathbb{C}$ such that
$$ \langle g,h\rangle_{D} + \epsilon \geq max_{c\in \mathbb{C}}\langle g, c\rangle_{D}. $$
We show that if the collection of subpopulation $\mathbb{C}$ admits an efficient agnostic learner, we could use that learner to construct an algorithm certifying multi-differential fairness.

\begin{thm}
\label{thm: al}
Let $\epsilon > 0$, $\beta >0$ and $\mathbb{C}\subset 2^{\mathfrak{X}}$. There exists an efficient $(\epsilon, \eta)$-auditing algorithm for $\mathbb{C}$ on balanced distributions if and only if $\mathbb{C}$ admits a $( \epsilon,\eta)$ efficient agnostic learner for any balanced distribution over $\mathfrak{A}$.  
\end{thm}

The result in theorem \ref{thm: al} makes clear that not all sub-population can be efficiently audited for multi-differential fairness. There are many concept classes $\mathbb{C}$ for which agnostic learning is a NP-hard problem, including for any learning methods that outputs a half-space as an hypothesis (see \cite{feldman2012agnostic}). However, there are classes for which efficient agnostic learners exist (see \cite{kearns1994toward}).

\bigskip
Based on theorem \ref{thm: al} and its proof,  we convert the certifying algorithm problem into the following empirical loss minimization: for a sample $\{(x_{i}, a_{i}), y_{i})\}_{i=1}^{m}\sim D^{w}_{f}$, solve

\begin{equation}
\label{eq: risk1}
  opt_{1}=\min_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i})\frac{1+y_{i}}{2}\right) 
\end{equation}

and 
\begin{equation}
\label{eq: risk2}
opt_{2}=\min_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i})\frac{1-y_{i}}{2}\right).
\end{equation}
The certifying algorithm \ref{algo: 1} delivers $c^{*}$, the solution  of the minimization problem corresponding to the smallest value of $opt_{1}$ and $opt_{2}$, as a $\hat{\gamma}$ unfairness certificate where $\hat{\gamma}=1-\min(opt_{1}, opt_{2})$. Generic uniform convergence argument allows to derive the sample complexity and correctness of our certifying algorithm \ref{algo: 1}. 

\begin{thm}(Sample Complexity and Correctness of Algorithm \ref{algo: 1})Let $\epsilon >0$ and $\eta\in(0,1)$. 
Suppose that $\mathbb{C}$ is a concept class of dimension $d(\mathbb{C})<\infty$. Algorithm \ref{algo: 1} is $(\epsilon, \eta)-$ certifying algorithm for samples of size $m\geq m(\epsilon, \eta, d)$, where
$$m=$$
\end{thm}

\begin{algorithm}
\caption{Certifying Algorithm}
\label{algo: 1}
\begin{algorithmic}[1]
\State \textbf{Input:}  $\{(x_{i}, f(x_{i})\}_{i=1}^{m}$, $\mathbb{C}\subset 2^{|\mathfrak{X}|}$
   
\State   {\bfseries Find:} 

$c^{*}_{1} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i})\frac{1+y_{i}}{2}\right) $ and $opt_{1}$

$c^{*}_{2} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i})\frac{1-y_{i}}{2}\right) $ and $opt_{2}$
 
\State $\hat{\gamma}\gets 1 - min\{opt_{1}, opt_{2}\}$
    
\State{\bfseries Return} $\hat{\gamma}-$ unfair
\end{algorithmic}
\end{algorithm}


\subsection{Unbalanced Data}
Algorithm \ref{algo: 1} certifies efficiently multi-differential fairness. The catch is that we assume that we have oracle access to importance sampling weights $w$. However, most of the time, importance sampling are unobserved and need to be estimated. Moreover, variance of those estimates are known to be large (see). In this section, we propose a noise infusing technique to certify multi-differential fairness without the need to estimate importance sampling weights.

\paragraph{Noise Infusion}
At issue with unbalanced distribution is that the multi-differential fairness condition in \eqref{eq: mdf_w} includes for $a\in\mathfrak{A}$ the term $w_{S}= Pr[A=a|S]/Pr[A\neq a|S]$:

\begin{equation}
    \label{eq: mdf_nw}
    Pr_{w}[A=a |S, y] \leq \frac{e^{\delta}w_{S}}{e^{\delta} w_{S} + 1},
\end{equation}
Therefore, algorithm \ref{algo: 1}, when applied to unbalanced distribution, cannot distinguish the case of high value of $\delta$ from the case of low value $\delta$ but a high value of $w_{S}$. The latter situation is the result of unbalance in the data that could result from social, cultural or historical biases; the former is an issue with the classifier $f$ itself that needs to be audited for. 






At a high level, algorithm \ref{algo: 2} introduces iteratively an increasing amount of noise that is adversarial to sub-populations $S$ with  high data imbalance (i.e. large $w_{S}$). For $\beta\in (0, 1/2)$, a distribution $D_{f}(\beta)$ is obtained from $D_{f}$ by the following transformation:for any $((x_{i}, a_{i}), y_{i})\sim D_{f}$, with probability $1-\beta$, set $a=a_{i}$ ; with probability $\beta$, flip a coin and set $a=a_{i}$ if head and $a=a^{'}$ with $a^{'}\neq a$ if tail. 

\bigskip
Because the transformation is independent of the classifier $f$'s outcome $y$, for any sub-population $S$,

\begin{equation}
    \frac{Pr_{\beta}[Y|A=a, S]}{Pr_{\beta}[Y|A\neq a, S]}= \frac{Pr[Y|A=a, S]}{Pr[Y|A\neq a, S]},
\end{equation}
where the subscript $\beta$ indicates that the probabilities are taken over the $D_{f}(\beta)$ distribution. Therefore, if for a sub-population $S\in \mathbb{C}$, there is a $\delta-$ multi differential violation with the distribution $D_{f}$, there is also a $\delta-$  multi differential violation with the distribution $D_{f}(\beta)$. On the other hand, for any $S\in \mathbbm{C}$,

\begin{equation}
    w_{S}(\beta) =\frac{Pr_{\beta}[A=a|S]}{Pr_{\beta}[A\neq a|S]} = \frac{\frac{1}{2}\beta + (1-\beta)Pr[A=a|S]}{\frac{1}{2}\beta + (1-\beta)Pr[A\neq a|S]}.
\end{equation}
The main properties of $w_{S}(\beta)$ are that $\frac{\partial w_{S}(\beta)}{\partial \beta}$ has the sign of $1-w_{S}$ and that $\frac{\partial^{2} w_{S}(\beta)}{\partial \beta w_{S}} < 0$.



\begin{algorithm}
\caption{Certifying Algorithm - Noise Infusion}
\label{algo: 2}
\begin{algorithmic}[1]
\State \textbf{Input:}  $\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$, $\mathbb{C}\subset 2^{|\mathfrak{X}|}$, $s$, $tol$
 
 \State $t=0$; $\gamma_{-1}=0$; $\gamma_{0}=1$ 
 
\While {$\abs{\gamma_{t} - \gamma_{t-1}} > tol$}  

\State $st$- noise infusion to transform $\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$ into $\{((x_{i}, a_{i}(st)), y_{i})\}_{i=1}^{m}$ 

\State $c^{*}_{1} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i}(st))\frac{1+y_{i}}{2}\right) $ and $opt_{1}$ 

\State $c^{*}_{2} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i}(st))\frac{1-y_{i}}{2}\right) $ and $opt_{2}$
 
 \State $i \gets argmin_{j=1,1}\{opt_{1}, opt_{2}\}$
\State $\hat{\gamma}_{t}\gets \frac{Pr[A=a|c_{i}^{*}=1]Pr[c_{i}^{*}=1, y]}{ w_{c^{*}==1}} $
 
 \State $t\gets t +1$
 
 \EndWhile   
\State{\bfseries Return} $\hat{\gamma}-$ unfair
\end{algorithmic}
\end{algorithm}


\subsection{Fairness Diagnostic}
Algorithm \ref{algo: 1} presented above allows to certify whether any black box classifier is multi-differential fair with only $O(\log(|\mathbb{C}|)$ samples. However, it does not identify the sub-population in $\mathbb{C}_{\alpha}$ with the strongest violation of multi differential fairness (i.e. with the largest value $\delta$ in \ref{def: mdf}). This is because algorithm \ref{algo: 1} does not distinguish a large sub-population $S$ with low value of $\delta$ from a smaller sub-population with larger value of $\delta$. Finding the strongest violation is useful to (i) diagnostic the source of multi-differential unfairness of a classifier; and, (ii) create methods that ensure multi-differential fairness. 

\paragraph{Worst Violation Problem}
The objective is to identify for any $a\in \mathfrak{A}$ the sub-population $S$ in $\mathbb{C}$ that solves:
\begin{equation}
\label{eq: wvio}
    \delta_{m} \equiv \max_{S\in \mathbb{C}}\log\left(\left.\frac{Pr[A=a|S, y]}{Pr[A\neq a|A, y]}\middle/\frac{Pr[A=a|S]}{Pr[A\neq a|S]}\right)\right. .
\end{equation}
To illustrate the challenges of the worst-violation problem, consider the case where there exists $S_{0}, S_{\delta}\in \mathbb{C}$ and a protected group $a\in \mathfrak{A}$ with no violation of multi differential fairness in $S_{0}$ (i.e. $0-$ multi differential fairness and $\delta-$ multi differential fairness violation in $S_{\delta}$ ($\delta >0$) when $Y=1$.  Consider $c, c^{'}\in \mathbb{C}$ such that $c(x)=1$ if and only if $x\in S_{\delta}$ and $c^{'}(x)=1$ if and only if $x\in S_{\delta}\cup S_{0}$. Both $c$ and $c^{'}$ have the same accuracy on $S_{\delta}\cup S_{0}$ if the distribution is balanced. Therefore, algorithm \ref{algo: 1} will pick indifferently $c$ or $c^{'}$ as unfairness certificate, although $c^{'}$ does not single out $S_{\delta}$ as a worst violation. 

\paragraph{Worst Violation Algorithm}
At issue in the previous example is that for sub-population no violation of multi differential fairness, choosing $c=1$ or $c=-1$ will lead to same empirical risk used in \ref{algo: 1}. Algorithm \ref{algo: 3} puts a slightly larger weight on samples $((x_{i}, a_{i}), y_{i})$ whenever $a_{i}\neq a$ so that the empirical risk is now smaller when choosing $c=-1$ whenever there is no violation of multi differential fairness.  More generally, at each iteration, the weight on samples $((x_{i}, a_{i}), y_{i})$ whenever $a_{i}\neq a$ is increased to $1 + \nu t$, with $\nu > 0$ and the solution $c_{t}^{*}$ of the empirical risk minimization \eqref{eq: risk1} or \eqref{eq: risk2} will identify $S\in \mathbb{C}$ as a violation only if $\delta \geq \log(1 + \nu t)$. The algorithm \ref{algo: 3} terminates whenever either $|S|\leq \alpha$ or $c^{*}_{t}(x)=1$ for all samples. At the last iteration before termination, theorem \ref{thm: algo3_ana} shows that algorithm \ref{algo: 3} will identify a sub-population with a $\delta$-multi differential fairness violation such that

\begin{equation}
\delta \geq \delta_{m} - \tilde{O}\left(\frac{1}{\sqrt{m}}\right).
\end{equation}

\begin{thm}
\label{thm: algo3_ana}
Suppose $\nu > 0, \epsilon >0, \eta\in (0, 1)$ and $\mathbb{C}\subset 2^{\mathfrak{X}}$ is $\alpha-$strong. Denote $\delta_{m}$ the worst violation of multi differential fairness for $\mathbb{C}$ as defined in \eqref{eq: wvio}. With probability $1-\eta$, with $O\left(\right)$ samples and after $O\left(\right)$ iterations, algorithm \ref{algo: 3} learns $c\in \mathbb{C}$ such that 
\begin{equation}
    \log\left(\frac{Pr_{w}[A=a|y, c(x)=1]}{Pr_{w}[A\neq a|y, c(x)=1]} \right) \geq \delta_{m} - \epsilon.
\end{equation}
\end{thm}
\begin{algorithm}
\caption{Worst Violation Algorithm}
\label{algo: 3}
\begin{algorithmic}[1]
\State \textbf{Input:}  $\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$, $\mathbb{C}\subset 2^{|\mathfrak{X}|}$, $s$, $tol$
 
 \State $t=0$; $\gamma_{-1}=0$; $\gamma_{0}=1$ 
 
\While {$\abs{\gamma_{t} - \gamma_{t-1}} > tol$}  

\State $st$- noise infusion to transform $\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$ into $\{((x_{i}, a_{i}(st)), y_{i})\}_{i=1}^{m}$ 

\State $c^{*}_{1} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i}(st))\frac{1+y_{i}}{2}\right) $ and $opt_{1}$ 

\State $c^{*}_{2} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i}(st))\frac{1-y_{i}}{2}\right) $ and $opt_{2}$
 
 \State $i \gets argmin_{j=1,1}\{opt_{1}, opt_{2}\}$
\State $\hat{\gamma}_{t}\gets \frac{Pr[A=a|c_{i}^{*}=1]Pr[c_{i}^{*}=1, y]}{ w_{c^{*}==1}} $
 
 \State $t\gets t +1$
 
 \EndWhile   
\State{\bfseries Return} $\hat{\gamma}-$ unfair
\end{algorithmic}
\end{algorithm}

\section{Experiments}

\subsection{Synthetic Data}
A synthetic data is constructed by drawing independently two features $X_{1}$ and $X_{2}$ from two standardized Gaussians and labelling the data according to the sign of $X_{1} + X_{2}$.   There is one protected attribute with binary values: $\mathfrak{A}=\{0, 1\}$. The audited classifier $f$ is a logistic regression classifier that is altered to generate instances of metric-free individual unfairness: with probability $\nu\in[0, 1)$, the sign of the outcomes from individuals with $A=0$ is changed from $-1$ to $+1$ and from $+1$ to $-1$. For $\nu=0$, the audited classifier is metric-free individual fair; however, as $\nu$ increases, for any subset of the feature spaces, there is a fraction  $\nu$ of individuals with protected value equal to $0$ who are not treated similarly as individuals with protected value equal to $1$. 

In Figure , the auditing algorithm $AMF2$ is trained using a logistic regression classifier. Figure shows that $AMF2$ delivers a $\gamma-$ unfairness certificate with a level of unfairness $\gamma$ approximately equal to the true level of unfairness $\nu$. The result confirms the theoretical results from theorem: $\nu$ is bounded below by $\gamma$ and $AMF2$'s accuracy increases with sample size $N$. when $N$ is equal to $20,000$, the level of unfairness detected by $AMF2$ aligns almost perfectly with the true level of unfairness $\nu$.  

In Figure , the auditing algorithm $AMF2$ is trained using a logistic regression  classifier, a decision tree classifier 


\section{Appendix}

\subsection{Analysis of Algorithm 2}

Let $a\in\mathfrak{A}$. Without loss of generality we consider the case where $Y=1$. At each iteration $t$, denote $c_{t}^{*}$ the solution of the following optimization problem 
\begin{equation}
\label{eq: opt1}
max_{c\in \mathbb{C}} E_{D_{f}^{w}}\left[\displaystyle\sum_{i=1}^{m} w_{it}\mathbbm{1}_{a}(a_{i})c(x_{i})\middle | Y=1\right],
\end{equation}
where $w_{it}$ are the weights at iteration $t$ and the expectation is taken over all the samples of size $m$ drawn from $D_{f}^{w}$.
\begin{equation}
    w_{it} = \begin{cases}
    w_{i}(1 + \nu t) \mbox{ if } a_{i}\neq a \\
    w_{i} \mbox{ otherwise.}
    \end{cases}
\end{equation}


For $c \in \mathbb{C}$, denote $\beta_{c}=E_{c=1, y=1}[\mathbbm{1}_{a}(a_{i})]$. Let $c_{0}\in \mathbb{C}$ such that $c_{0}(x)=-1$ for all $x\in \mathfrak{X}$. 
Denote $B_{c}=\{x_{i}| g(x_{i})=1 \; f_{a}(x_{i}) =f_{a^{'}}(x_{i})\}$, $B_{g}^{+}=\{x_{i}| g(x_{i})=1 \; f_{a}(x_{i})  >f_{a^{'}}(x_{i})\}$ and $B_{g}^{-}=\{x_{i}| g(x_{i})=1 \; f_{a}(x_{i}) <f_{a^{'}}(x_{i})\}$. Observe that 
\begin{equation}\beta_{g}= \left| \displaystyle\sum_{i, x_{i}\in B_{g}^{+}}w_{i} - \displaystyle\sum_{i, x_{i}\in B_{g}^{-}}w_{i} \right|.
\end{equation}
Assume first that $\displaystyle\sum_{i, x_{i}\in B_{g}^{+}}w_{i} > \displaystyle\sum_{i, x_{i}\in B_{g}^{-}}w_{i}$. Therefore,

\begin{equation}
\begin{split}
   E_{D}\left[\displaystyle\sum_{i=1, g(x_{i})=1}^{m} w_{it}f_{i}^{*}g(x_{i})\right] &=  E_{D}\left[\displaystyle\sum_{i=1, x_{i}\in B_{g} }^{m} w_{it}f_{i}^{*} + \displaystyle\sum_{i=1, x_{i}\notin B_{g} }^{m} w_{it}f_{i}^{*}\right] \\
   & = -\frac{1}{2}\epsilon t E_{D}[|B_{g}|] - 
   \epsilon t  E_{D}[|B_{g}^{-}|] -  E_{D}\left[ \displaystyle\sum_{i=1, x_{i}\in B_{g}^{-} }^{m} w_{i}\right]  \\
   & +  E_{D}\left[ \displaystyle\sum_{i=1, x_{i}\in B_{g}^{+} }^{m} w_{i}\right] \\
   & = -\frac{1}{2}\epsilon t E_{D}[|B_{g}|]- 
   \epsilon t  E_{D}[|B_{g}^{-}|] + \beta_{g},
   \end{split}
\end{equation}
where $|B|=\displaystyle\sum_{i=1, x_{i}\in B}^{m}w_{i}$ for any $B\in \mathbb{C}$. Moreover, 
\begin{equation}
   E_{D}\left[\frac{1}{m}\displaystyle\sum_{i=1, g(x_{i})=1}^{m} w_{it}f_{i}^{*}g_{0}(x_{i})\right] = \frac{1}{2}\epsilon t E_{D}[|B_{g}|] +
   \epsilon t  E_{D}[|B_{g}^{-}|]- \beta_{g}.
\end{equation}
Therefore, $g$ cannot be a solution of \eqref{eq: opt1} if 
\begin{equation}
    \epsilon t\left( E_{D}[|B_{g}|] +
   2  E_{D}[|B_{g}^{-}|]\right) > 2\beta_{g}.
\end{equation}
Note that $|B_{g}| = 1 -\left( |B_{g}^{-}| + |B_{g}^{+}|\right)$ and $\beta_{g}=|B_{g}^{+}| -  |B_{g}^{-}|$. Therefore, $g$ cannot be a solution of \eqref{eq: opt1} if

\begin{equation}
    t > \frac{2\beta_{g}}{\epsilon(1-\beta_{g})}
\end{equation}
At any iteration $t$, a solution of \eqref{eq: opt1} is either $g(x)=-1$ for all $x\in\mathfrak{X}$ or $\beta_{g} > \frac{\epsilon t}{\epsilon t + 2}$. 

\bigskip
\textbf{Small samples properties}
We can use a generic uniform convergence property: 

\begin{thm}
\label{thm1}
Let$\mathfrak{H}$ be a family of function  mapping from $\mathfrak{X}$ to $\{-1, 1\}$ and let $S=\{x_{1}, ..., x_{m}\}$ be a sample where $x_{i}\sim D$ for some distribution $D$ over $\mathfrak{X}$. With probability $1-\delta$, for all $h\in \mathfrak{H}$
$$  \left|E_{S\sim D}[h]- \frac{1}{m}\displaystyle\sum_{i=1}^{m} h(x_{i})\right| \leq 2\mathfrak{R}_{m}(\mathfrak{H}) + \sqrt{\frac{2\ln(1/\delta)}{m}}.$$
\end{thm}

Applying the uniform convergence result from \ref{thm1} allows deriving property of algorithm 2. For any a sample $S$ and any $g\in \mathbb{C}$ with probability $1-\delta/2$, ,
\begin{equation}
    \left|\frac{1}{m}\displaystyle\sum_{i=1, g(x_{i})=1}^{m} w_{it}f_{i}^{*}g(x_{i}) + \frac{1}{2}\epsilon t E_{D}[|B_{g}|] +
   \epsilon t  E_{D}[|B_{g}^{-}|] - \beta_{g}\right| \leq 2\mathfrak{R}_{m}(\mathfrak{\mathbb{C}}) + \sqrt{\frac{2\ln(2/\delta)}{m}},
\end{equation}

and 
\begin{equation}
    \left|\frac{1}{m}\displaystyle\sum_{i=1, g(x_{i})=1}^{m} w_{it}f_{i}^{*}g_{0}(x_{i}) - \frac{1}{2}\epsilon t E_{D}[|B_{g}|] -
   \epsilon t  E_{D}[|B_{g}^{-}|] + \beta_{g}\right| \leq 2\mathfrak{R}_{m}(\mathfrak{\mathbb{C}}) + \sqrt{\frac{2\ln(2/\delta)}{m}}.
\end{equation}
Therefore, with probability $1-\delta$, $h$ cannot be solution of the empirical counterpart of \eqref{eq: opt1} if 
\begin{equation}
     t > \frac{2\beta_{g}}{\epsilon(1-\beta_{g})} + \frac{4}{1-\beta_{g}}\mathfrak{R}_{m}(\mathfrak{\mathbb{C}}) + \frac{2}{1-\beta_{g}}\sqrt{\frac{2\ln(2/\delta)}{m}}.
\end{equation}
Therefore at iteration $t$, with probability $1-\delta$, a solution of \eqref{eq: opt1} for a sample $S$ is either $g(x)=-1$ for all $x\in S$ or 
\begin{equation}
    \beta_{g} > \frac{\epsilon t}{2 + \epsilon t} - \frac{4}{2 + \epsilon t}\mathfrak{R}_{m}(\mathfrak{\mathbb{C}}) - \frac{2}{2 + \epsilon t}\sqrt{\frac{2\ln(2/\delta)}{m}}.
\end{equation}


\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{references}

\end{document}