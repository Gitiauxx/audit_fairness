\documentclass{article}

\usepackage{mathtools}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\usepackage[utf8]{inputenc}
\usepackage{pgfplots}

\pgfplotsset{
    discard if not/.style 2 args={
        x filter/.code={
            \edef\tempa{\thisrow{#1}}
            \edef\tempb{#2}
            \ifx\tempa\tempb
            \else
                \def\pgfmathresult{inf}
            \fi
        }
    }
}

\usepackage{subcaption}
\usepackage{gensymb}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\usepackage{enumerate}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}
\newtheorem{thm}{Theorem}[section]
\newtheorem*{thmt*}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{prop}[thm]{Proposition}
\pgfplotsset{compat=1.15}

\newtheorem{defn}{Definition}[section]
\title{Auditing for Multi-Differential Fairness of Black Box Classifiers}
\author{}
\date{January 2019}

\begin{document}
\maketitle
\section{Introduction}
Machine learning algorithms are used to make support decisions that have adverse consequences on an individual's life: for example, classifiers have used to support the judicial when deciding whether a criminal offender is likely to recommit a crime; or lender to determine the default risk of a potential borrower. At issue is whether classifiers are fair in the sense of \cite{calsamiglia2009decentralizing} , that is whether classifiers' outcomes are independent of \textit{exogenous irrelevant characteristics} (\cite{calsamiglia2009decentralizing}) or protected attributes, including race and gender. Abundant examples of classifiers' discrimination can be found in ... 

\bigskip
The question is how much additional harm does a black box classifier whose design, data and validation procedures are unknown. If the models have been tinkered with to obtain specific results or maliciously or inadvertently. Notion of constability (Hirsch 2017) and ability to reason about the classifier's outcomes. Different impact versus different treatment? In Ricci vs Stefano, the Suppreme upheld the results of the tests. Are we looking at different treatment instead of different impacts?

Disparate impacts paper: they estimate whether we can predict protected attributes using data $D$. We argue for a disparate treatment measure that compare the additional disparate impacts caused by the classifier's outcome. 

\bigskip
Ricci and Destefano: " liable for disparate impact discrimination only if the exams at issue were not job related and consistent with business necessity, ...." can we link this to choosing relevant auditing features, that are related/consistent with the task at play.

Higher priority of disparate treatment: "the City can avoid disparate-impact liability based on
the strong basis in evidence that, had it not certified the results, it
would have been subject to disparate-treatment liability"



\bigskip
However, despite a growing literature on classifier's fairness, there are at least two limitations to a systematic approach in defining what classifier means [frame this in terms of bounds: a computation bound and a data/social bound]. First, classifiers are trained and/or audited using samples from distribution that are not independent across external characteristics like race or gender. In fact, we show in this paper that for every classifier with non-trivial accuracy, there exists an imbalanced distribution for which the classifier's outcome will depend on protected attributes for some individuals (see also \cite{kleinberg2016inherent}).This forms the first bound to classifier's fairness, a social bound. Secondly, aggregate notion of fairness that guarantees some definition of fairness to hold on average is not sufficient, since it can hide significant unfairness at the individual level. But only sub-populations that can be efficiently computed can be audited for classifier's fairness. This forms a so-called computational bound to fairness.

\bigskip
This paper introduces a notion of individual differential fairness that makes explicit the information-theory bound. Individual differential fairness is a guarantee that a classifier's outcomes are nearly mean-independent of protected attributes conditional on an individual's features. Borrowing from the differential privacy literature, this definition can be interpreted as a privacy guarantee: a classifier's outcome will leak negligible information about the distribution of protected attributes among individuals sharing the same non-protected features. 


Notes: race as a social/legal construct... even with identical feature, two individuals identified in different races will have a different experiment. Directly deal with the social embeddedness of race. Power to communities to understand what the algorithm does. Issues with counterfactual: what is the counterfactual? Yourself with white parents? richer parents.....? It stipulates the source of unfairness but it does not help ... Can AI account for social embeddedness?

Fairness: impartial and just treatment without discrimination or favoritism. Bias and fairness is not interchangeable. But bias is a feature of statistical models; fairness is a feature of human value judgments. Better question: when is fairness instead of what is fairness? 

Look into influence functions and Mitigating unwanted biases with adversarial learning

Challenges to formal philosophical models: high unfairness on a small population versus spread of little of unfairness?

Practical applications......... and due diligence


\section{Multi-Differential Fairness}

\subsection{Preliminary}

\paragraph{Notations}
An individual $i$ is defined by a tuple $((x_{i}, a_{i}), y_{i})$, where $x_{i}\in \mathfrak{X}$ denotes individual $i$'s audited features; $a_{i}\in\mathfrak{A}$ denotes her protected attribute; and $y_{i}\in \{-1, 1\}$ is the classification provided by a black box classifier $f$. The auditor draw samples $
\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$ of size $m$ from a distribution $D$ on $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$. 

\bigskip
Features in $\mathfrak{X}$ are not necessarily the ones used to train $f$. First, the auditor may not have access to all features used to train $f$. Secondly, the auditor may decide to leave deliberately some features used to train $f$ out of $\mathfrak{X}$ because she believes that those features should not be used to define similarity among individuals. For example, if $f$ classifies loan according to their probability of repayment, the auditor may consider that credit score should be used to define individual similarity, but that zipcode, because correlated with races, should not be an auditing feature, although it was used to train $f$.  

\paragraph{Assumptions}
In our analysis we make the following assumption:
\begin{assumption}
\label{ass: 1}
For all $x\in \mathfrak{X}$, $Pr[A|X=x] > 0.$
\end{assumption}
Assumption \ref{ass: 1} guarantees that the distribution of auditing features conditional on protected attributes have common support: there is no $x\in \mathfrak{X}$ that reveals perfectly the individual's protected attribute.  

\subsection{Individual Differential Fairness}
\paragraph{Individual Differential Fairness} 
We define differential fairness as the guarantee that a classifier leaks with negligible probability the protected attribute of an individual.

\begin{defn}(Individual Differential Fairness)
\label{def: idf}
For $\delta \in [0, 1)$, a classifier $f$ is $\ \delta-$ differential fair if for all $x\in\mathfrak{X}$ and all $a\neq a^{'}\in \mathfrak{A}$
\begin{equation}
\label{eq: idf}
    Pr[Y|a, x] \leq e^{\delta}Pr[Y|a^{'}, x]
\end{equation}
\end{defn}

The parameter $\delta$ controls the amount of information leaked by $f$ on the distribution of protected attributes of an individual with auditing feature $x$: larger value of $\delta$ implies larger leakage. For example, for a classifier that does not satisfy the fairness condition \ref{def: idf} for some $\delta=\ln(2)$, there exist individuals $x$ that are twice as likely to be classified $y=1$ if $A=1$ than if $A=-1$. In that example, the classifer's outcome $y$ leaks information related to $A$ that were not leaked by $x$ alone.   

\paragraph{Disparate Treatment versus Disparate Impact}
The advantage of individual differential fairness is to distinguish the unfairness caused by $f$  from the one already embedded in the data. To illustrate this point, note that the fairness condition in \ref{def: idf} is equivalent to:
\begin{equation}
    \frac{Pr[A=a|x, y]}{Pr[A=a^{'}|x, y]} \leq e^{\delta}\frac{Pr[A=a|x]}{Pr[A=a^{'}|x]}.
\end{equation}
The right hand side ratio characterizes the information leaks by feature $x$ on protected attributes. For example, a ratio larger than one indicates that an individual with feature $x$ is more likely to be of protected attribute $A=a$: zipcodes of neighborhoods densely populated with a minority.  The issue is whether the classifier exacerbates the leakage. Auditing features might strongly correlate with protected attributes (e.g. zipcodes densely populated by a minority) because the data reflects social, cultural and historical biases. Individual differential fairness imposes a restriction on the classifier $f$ to not exacerbate those biases. 

\bigskip
One issue is whether the classifier can undo some of the leakages occurring via the features $x$.  The answer is no, unless the data is balanced, i.e $Pr[A=a|x] = Pr[A\neq a]| x]$. 

\begin{thm}
Let $x\in \mathfrak{X}$ such that $Pr[A=a|x] /Pr[A\neq a] > 1 + \mu$ with $\mu \geq 0$. Then,

\begin{equation}
 \frac{Pr[A=a|x, y]}{Pr[A=a^{'}|x, y]} < 1 + \mu \iff \frac{Pr[Y=y|x, A\neq a]}{Pr[Y=y|x, A=a]} > 1
 \end{equation}
\end{thm}

[placeholder: how this result relates to \cite{kleinberg2016inherent}] The result has negative consequences for fairness in classification. If feature $x$ leaks information related to the protected attribute $A$, then the classifier's outcomes cannot be $0$-individual differential fair if the classifier reduces the leakage occurring through $x$. 

\paragraph{Relation with differential privacy}
There is an analogy between individual differential fairness for classifiers and differential privacy for database queries. Differential privacy as in  \cite{dwork2014algorithmic} guarantees that outcomes from a query are not distinguishable identical when computed on two adjacent databases that differs only by one record. The fairness condition \eqref{eq: idf} implies that outcomes from a classifier are not distinguishable for individuals that differ only by their protected attributes. [placeholder: why does it matter? Possibly, (i) merges the field of fairness with privacy, a field where computer science is "more comfortable" with; (ii). Why is there no balance issue in differential privacy? ] 

\paragraph{Individual Fairness}
The definition \ref{def: idf} is an individual level definition of fairness. Compared to the notion of individual fairness in \cite{dwork2012fairness}, individual differential fairness does not require to explicit a similarity metric. This is important because defining a similarity metric has been the main limitation of applying the concept of individual fairness.

\paragraph{Other notion of fairness}
need to look at disparate impact (an aggregate version of \ref{def: idf}. 

\subsection{Multi-differential fairness}
Although useful, the notion of individual differential fairness suffers from one limitation: it cannot be computationally efficiently audited for. Looking for violations of individual differential fairness will require searching over a set of $2^{|\mathfrak{X}|}$ individuals. Moreover, if $\mathfrak{X}$ is rich enough empirically, a sample from a distribution over $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$ has a negligible probability to have two individuals with the same auditing feature $x$ and different protected attributes $a$. 

\bigskip
Therefore, we relax the definition of individual differential fairness and impose differential individual fairness for group of individuals or sub-populations. Formally, $\mathfrak{C}$ denotes a collection of subsets $S$ of $\mathfrak{X}$. The collection $\mathfrak{C}_{\alpha}$ is $\alpha$-strong if for $S\in \mathfrak{C}$ and $y\in \{-1, 1\}$, $Pr[Y=y \;\&\; x\in S] \geq \alpha$.  

\begin{defn}(Multi-Differential Fairness)
\label{def: mdf}
Consider a $\alpha$-strong collection $\mathbb{C}_{\alpha}$ of sub-populations of $\mathfrak{X}$. For $0\leq \delta$, a classifier $f$ is $(\mathbb{C}_{\alpha}, \delta)$-multi differential fair with respect to $\mathfrak{A}$ if for all protected attributes $a, a^{'}\in \mathfrak{A}$, $y\in\{-1, 1\}$ and for all $S\in \mathbb{C}_{\alpha}$:
\begin{equation}
\label{eq: mdf}
Pr[Y=y|A=a, S] \leq e^{\delta} Pr[Y=y|A=a^{'}, S]
\end{equation}
\end{defn}

Multi-differential fairness relaxes the notion of differential fairness by protecting sub-populations instead of individuals. Multi-differential fairness guarantees that the outcome of a classifier $f$ is nearly mean-independent of protected attributes within any sub-population $S\in \mathfrak{C}_{\alpha}$. The parameter $\delta$ controls for the amount of information related to protected attributes that the classifier leaks: smaller value of $\delta$ means smaller leakage. The fairness condition \ref{eq: mdf} applies only to subpopulations with $Pr[Y=y \;\&\; x\in S] \geq \alpha$ for $y\in\{-1, 1\}$. This is to avoid trivial cases where $\{x\in S \; \& \; Y=y\}$ is a singleton for some $y$, which would imply that $\delta=\infty$. 


\paragraph{Collection of Indicators.}
The collection of sub-population $\mathbb{C}$ can be equivalently thought as a family of indicators: for each $S\in \mathbb{C}$, there is an indicator $c_{S}: \mathfrak{X}\rightarrow \{-1, 1\}$ such that $c_{S}(x)=1$ if and only if $x\in S$. The relaxation of differential fairness to a collection of groups or sub-population is akin to \cite{kim2018fairness}, \cite{kearns2017preventing} or \cite{hebert2017calibration} where $\mathfrak{C}$ is the computational bound on the granularity of their definition of fairness. The richer $\mathbb{C}$, the stronger the fairness guarantee offers by definition \ref{def: mdf}. However, the complexity of $\mathbb{C}$ is limited by the fact that we identify a sub-population $S$ via random samples drawn from a distribution over $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$. The rest of this paper shows that auditing for multi-differential fairness in polynomial time requires to limit the complexity of $\mathbb{C}$. Potential candidates for $\mathbb{C}$ will be the family short-decision trees or the set of conjunctions of constant number of boolean features. Therefore, auditing for multi-differential fairness will not check whether the fairness condition \eqref{eq: mdf} holds for \textit{all} sub-populations of $\mathfrak{X}$, but only check  the fairness condition for all sub-populations that can be \textit{efficiently identifiable}. 

\section{Auditing, Agnostic Learning and PAC learning}

The definition of multi-differential fairness requires to verify that in no sub-population $S\in \mathbb{C}$ with $Pr[S]\geq \alpha$, the classifier leaks information about the distribution of protected attributes. If $\mathbb{C}$ is a rich and large class of subsets of the feature space $\mathfrak{X}$, an auditing algorithm linearly dependent on $|\mathbb{C}|$ can be prohibitively expensive. In this section we show that finding an auditing algorithm reduces to agnostic learning of the class of sub-populations $\mathbb{C}$. That is, there is no $\log(\mathbb{C})$ running time auditing algorithm unless $\mathbb{C}$ is efficiently agnostically learnable.  

\subsection{Certifying Fairness and Agnostic Learning}
Auditing for multi-differential fairness consists firstly, in establishing there exists a fairness violation; secondly, in identifying a sub-population $S$ that violates the most the fairness condition in \ref{def: mdf}. 

\paragraph{Multi Differential Fairness and Balanced Distribution}
The fairness condition \ref{def: mdf} is unchanged if the feature distribution is reweighted, as long as the reweighting scheme does not depend on the classifier's outcome $Y$. More formally, for any weights $u: \mathfrak{X}\times \mathfrak{A} \rightarrow \mathbb{R}$ such that $u(x,a)> 0$ and $E[u]=1$, 

\begin{equation}
Pr[Y|A=a, S] \leq e^{\delta} Pr[Y|A=a^{'}, S] \iff Pr_{u}[Y|A=a, S] \leq e^{\delta} Pr_{u}[Y|A=a^{'}, S],
\end{equation}
the sub-script $u$ indicating that the probability are taken over the reweighted distribution. 

\bigskip
Suppose that for any $a \in \mathfrak{A}$, we have oracle access to the importance sampling weight $w_{a}(x)=\frac{1 - P[A=a|x]}{P[A=a|x]}$. For any distribution $D_{f}$ over $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$ denote $D_{f}^{w}$ with $Pr_{w}[A|x]= 1 - Pr_{w}[A|x]$ the corresponding balanced distribution. Note that once reweighted by $w_{a}$, for any sub-population $S\in \mathbb{C}$, auditing features does not reveal anything about the distribution of the protected attribute $A$: $Pr_{w}[A=a|X, S]=Pr_{w}[A\neq a|X, S]$. With a balanced distribution, the multi differential fairness condition can be rewritten as follows: for all protected attributes $a\in \mathfrak{A}$, $y\in \{-1,1\}$ and for all $S\in \mathbb{C}_{\alpha}$

\begin{equation}
    \label{eq: mdf_w}
    Pr_{w}[A=a |S, y] \leq \frac{e^{\delta}}{e^{\delta} + 1},
\end{equation}
where the sub-script $w$ reminds that the distribution $D_{f}^{w}$ is balanced. Since the distribution $D_{f}^{w}$ induced by $f$ is balanced, auditing features $x$ do not reveal any information on protected attributes and multi-differential fairness can then be interpreted as an upper bound on ability to predict $A$ given the classifier's outcome for any sub-population $S\in \mathbb{C}$ with $Pr_{w}[S, y] \geq \alpha$ for $y\in\{-1, 1\}$.  A violation of $(\mathbb{C}_{\alpha}, \delta)$- multi differential fairness is a sub-population $S\in \mathbb{C}_{\alpha}$ such that 

\begin{equation}
    \label{eq: mdf_viol1}
    Pr_{w}[A=a |S, y] - \frac{1}{2}\geq \frac{e^{\delta}}{e^{\delta} + 1} - \frac{1}{2}. 
\end{equation}

Thereofre, a $\gamma$-unfairness certificate is a subset $S\in \mathbb{C}$ such that there exists $y\in \{-1, 1\}$ with
\begin{equation}
    Pr_{w}[S, y]\left\{Pr_{w}[A=a |S, y] -\frac{1}{2}\right\}\geq \gamma,
\end{equation}
with $\gamma =\alpha \left(e^{\delta}/(1+e^{\delta})-1/2\right)$. $\gamma$ is then a measure of multi-differential unfairness that combines the size of the sub-population where a violation exists and the magnitude of the violation. With balanced distribution, certifying multi-differential fairness is akin to searching for $\gamma$-unfairness certificate. 

\begin{defn}(Certifying Multi-Differential Fairness). 
Let $\gamma, \epsilon >0$, $\eta\in (0, 1)$ and $\mathbb{C}_{\alpha}$ be an $\alpha$-strong collection of sub-populations in $\mathfrak{X}$. An $(\epsilon, \eta)$- certifying algorithm $M(\epsilon, \delta)$ is an algorithm that for any sample from a distribution $D_{f}$ induced by a classifier over $\mathfrak{X} \times \mathfrak{A}\times \{-1, 1\}$, outputs a $\gamma-\epsilon$-unfairness certificate with probability $1-\eta$ whenever $f$ is $ \gamma$-multi differential unfair; and, certifies fairness with probability $1-\eta$ whenever $f$ is is $\gamma$-multi differential fair. 

\bigskip
Moreover, $M(\epsilon, \delta)$ is an efficient certifying algorithm if it requires $poly(\log(|\mathfrak{C}_{\alpha}|, \log(1/\eta), 1/\epsilon))$ samples and runs in $poly(\log(|\mathfrak{C}_{\alpha}|, \log(1/\eta), 1/\epsilon))$. 
\end{defn}

Searching for $\gamma$-unfairness certificate can be formulated as a problem of detecting correlations. For $g, h:\mathfrak{X}\rightarrow \{-1, 1\}$,   let $\langle g, h\rangle_{D}\equiv E_{D}g(x)h(x)$ denote the average inner product between $g$ and $h$. Then, $S$ is a $\gamma$-unfair certificate of $f$ if and only if there exists $y\in \{-1, 1\}$ such that
\begin{equation}
    \left\langle \frac{1+c_{S}}{2}, \frac{a + 1}{2}\middle|Y=y\right\rangle_{D_{w}^{f}} -\frac{1}{2} \left\langle \frac{1+c_{S}}{2}, 1\middle |Y=y\right\rangle_{D_{w}^{f}} \geq \gamma,
\end{equation}
or equivalently, since $2Pr_{w}[a=c] - 1= \langle a, c_{S}\rangle_{D_{w}^{f}}$

\begin{equation}
Pr_{w}[A=c_{S}| Y=y] \geq 1 - \rho(y) + 2\gamma
\end{equation}
with $\rho(y)=Pr_{w}[A=a|Y=y]$. Detecting multi-differential fairness violation can be phrased as a weak agnostic learning problem. A concept class $\mathbb{C}$ is agnostically efficiently learnable if and only if for all $\epsilon, \eta >0$, there exists an algorithm $\mathfrak{M}$ that given access to a distribution $\{x_{i}, o_{i}\}\sim D\times \{-1, 1\}$ outputs with probability $1-\eta$ in $poly(\log(|\mathfrak{C}, \log(\frac{1}{\delta}), \epsilon)$ outputs  a function $h\in \mathbb{C}$ such that
$$ \langle g,h\rangle_{D} + \epsilon \geq max_{c\in \mathbb{C}}\langle g, c\rangle_{D}. $$
We show that if the collection of subpopulation $\mathbb{C}$ admits an efficient agnostic learner, we could use that learner to construct an algorithm certifying multi-differential fairness.

\begin{thm}
\label{thm: al}
Let $\epsilon > 0$, $\beta >0$ and $\mathbb{C}\subset 2^{\mathfrak{X}}$. There exists an efficient $(\epsilon, \eta)$-auditing algorithm for $\mathbb{C}$ on balanced distributions if and only if $\mathbb{C}$ admits a $( \epsilon,\eta)$ efficient agnostic learner for any balanced distribution over $\mathfrak{A}$.  
\end{thm}

The result in theorem \ref{thm: al} makes clear that not all sub-population can be efficiently audited for multi-differential fairness. There are many concept classes $\mathbb{C}$ for which agnostic learning is a NP-hard problem, including for any learning methods that outputs a half-space as an hypothesis (see \cite{feldman2012agnostic}). However, there are classes for which efficient agnostic learners exist (see \cite{kearns1994toward}).

\bigskip
Based on theorem \ref{thm: al} and its proof,  we convert the certifying algorithm problem into the following empirical loss minimization: for a sample $\{(x_{i}, a_{i}), y_{i})\}_{i=1}^{m}\sim D^{w}_{f}$, solve

\begin{equation}
\label{eq: risk1}
  opt_{1}=\min_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i})\frac{1+y_{i}}{2}\right) 
\end{equation}

and 
\begin{equation}
\label{eq: risk2}
opt_{2}=\min_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i})\frac{1-y_{i}}{2}\right).
\end{equation}
The certifying algorithm \ref{algo: 1} delivers $c^{*}$, the solution  of the minimization problem corresponding to the smallest value of $opt_{1}$ and $opt_{2}$, as a $\hat{\gamma}$ unfairness certificate where $\hat{\gamma}=1-\min(opt_{1}, opt_{2})$. Generic uniform convergence argument allows to derive the sample complexity and correctness of our certifying algorithm \ref{algo: 1}. 

\begin{thm}
\label{thm: corr1}
(Sample Complexity and Correctness of Algorithm \ref{algo: 1})Let $\epsilon >0$ and $\eta\in(0,1)$. 
Suppose that $\mathbb{C}$ is a concept class of dimension $d(\mathbb{C})<\infty$. Algorithm \ref{algo: 1} is $(\epsilon, \eta)-$ certifying algorithm for samples of size $m\geq m(\epsilon, \eta, d)$, where
$$m=$$
\end{thm}

\begin{algorithm}
\caption{Certifying Algorithm}
\label{algo: 1}
\begin{algorithmic}[1]
\State \textbf{Input:}  $\{(x_{i}, f(x_{i})\}_{i=1}^{m}$, $\mathbb{C}\subset 2^{|\mathfrak{X}|}$
   
\State   {\bfseries Find:} 

$c^{*}_{1} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i})\frac{1+y_{i}}{2}\right) $ and $opt_{1}$

$c^{*}_{2} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i})\frac{1-y_{i}}{2}\right) $ and $opt_{2}$
 
\State $\hat{\gamma}\gets 1 - min\{opt_{1}, opt_{2}\}$
    
\State{\bfseries Return} $\hat{\gamma}-$ unfair
\end{algorithmic}
\end{algorithm}


\subsection{Unbalanced Data}
Algorithm \ref{algo: 1} certifies efficiently multi-differential fairness. The catch is that we assume that we have oracle access to importance sampling weights $w$. However, most of the time, importance sampling are unobserved and need to be estimated. Moreover, variance of those estimates are known to be large (see). In this section, we propose a noise infusing technique to certify multi-differential fairness without the need to estimate importance sampling weights.

\paragraph{Importance Sampling and Reweighting}
At issue with unbalanced distribution is that the multi-differential fairness condition in \eqref{eq: mdf_w} includes for $a\in\mathfrak{A}$ the term $w_{S}= Pr[A=a|S]/Pr[A\neq a|S]$:

\begin{equation}
    \label{eq: mdf_nw}
    Pr_{w}[A=a |S, y] \leq \frac{e^{\delta}w_{S}}{e^{\delta} w_{S} + 1},
\end{equation}
Therefore, algorithm \ref{algo: 1}, when applied to unbalanced distribution, cannot distinguish the case of high value of $\delta$ from the case of low value $\delta$ but a high value of $w_{S}$. The latter situation is the result of unbalance in the data that could result from social, cultural or historical biases; the former is an issue with the classifier $f$ itself that needs to be audited for. 

\bigskip
One approach to obtain $w$ is to directly estimate the density $P[A=a|x]$. This idea of importance sampling is used in propensity-score matching methods (see) in the context of counterfactual analysis. However, exact or estimated importance sampling result in large variance in finite sample. In fact, estimating the distribution $P[A=a|x]$ to obtain the weight $w_{a}(x)$ may be an overkill. \cite{gretton2009covariate} shows that the weights can be obtained by minimizing the following maximum mean discrepancy problem

\begin{equation}
\label{eq: mmd1}
\left \Vert \frac{1}{n_{a}}\displaystyle\sum_{i, A=a}w_{a}(x)\phi(x) -\frac{1}{n_{a^{'}}}\displaystyle\sum_{i, A=a^{'}}\phi(x)\right \Vert^{2}
\end{equation}
where the discrepancy is measured over all function in the RKHS represented by the kernel $k(x,x^{'})=\langle \phi(x)| \phi(x^{'})\rangle$. Our approach $MMD$ takes $\phi(x)=x$ and obtains $w_{a}$ by solving 

\begin{equation}
\min_{w\in \mathbb{W}}\left \Vert \frac{1}{n_{a}}\displaystyle\sum_{i, A=a}w(x)x -\frac{1}{n_{a^{'}}}\displaystyle\sum_{i, A=a^{'}}x\right \Vert^{2} + Reg(w)
\end{equation}
where $\mathbb{W}$ is a concept class for the weight function and $Reg(w)$ is a regularization parameter for $w$. 
 

\bigskip
 The last approach is to regularize further the maximum mean discrepancy method  by attempting to extract only the information predictive of the auditing algorithm outcomes $ay$. We jointly learn a representation $\phi$ of the feature space and a weight function $w(\phi)$ by minimizing the following loss:
 
\begin{equation}
\begin{split}
L(w, c, \phi) = &\frac{1}{n_{a}+n_{a^{'}}}\displaystyle\sum_{i}w_{i}\mathbbm{1}(c(\phi(x_{i}))\neq y^{*}_{i}) + Reg(c)  \\
& + \left \Vert \frac{1}{n_{a}}\displaystyle\sum_{i, A=a}w(x)\phi(x) -\frac{1}{n_{a^{'}}}\displaystyle\sum_{i, A=a^{'}}\phi(x)\right \Vert^{2} + Reg(w)
\end{split}.
\end{equation}
In our implementation $DNN-MMD$, the common representation $\phi$ is learned via a neural network that is then shared with both tasks of minimizing the re-weighted auditing risk and the distributional shift between features distribution for the minority and majority group. 







\begin{algorithm}
\caption{Certifying Algorithm - Noise Infusion}
\label{algo: 2}
\begin{algorithmic}[1]
\State \textbf{Input:}  $\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$, $\mathbb{C}\subset 2^{|\mathfrak{X}|}$, $s$, $tol$
 
 \State $t=0$; $\gamma_{-1}=0$; $\gamma_{0}=1$ 
 
\While {$\abs{\gamma_{t} - \gamma_{t-1}} > tol$}  

\State $st$- noise infusion to transform $\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$ into $\{((x_{i}, a_{i}(st)), y_{i})\}_{i=1}^{m}$ 

\State $c^{*}_{1} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i}(st))\frac{1+y_{i}}{2}\right) $ and $opt_{1}$ 

\State $c^{*}_{2} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i}(st))\frac{1-y_{i}}{2}\right) $ and $opt_{2}$
 
 \State $i \gets argmin_{j=1,1}\{opt_{1}, opt_{2}\}$
\State $\hat{\gamma}_{t}\gets \frac{Pr[A=a|c_{i}^{*}=1]Pr[c_{i}^{*}=1, y]}{ w_{c^{*}==1}} $
 
 \State $t\gets t +1$
 
 \EndWhile   
\State{\bfseries Return} $\hat{\gamma}-$ unfair
\end{algorithmic}
\end{algorithm}


\subsection{Fairness Diagnostic}
Algorithm \ref{algo: 1} presented above allows to certify whether any black box classifier is multi-differential fair with only $O(\log(|\mathbb{C}|)$ samples. However, it does not identify the sub-population in $\mathbb{C}_{\alpha}$ with the strongest violation of multi differential fairness (i.e. with the largest value $\delta$ in \ref{def: mdf}). This is because algorithm \ref{algo: 1} does not distinguish a large sub-population $S$ with low value of $\delta$ from a smaller sub-population with larger value of $\delta$. Finding the strongest violation is useful to (i) diagnostic the source of multi-differential unfairness of a classifier; and, (ii) create methods that ensure multi-differential fairness. 

\paragraph{Worst Violation Problem}
The objective is to identify for any $a\in \mathfrak{A}$ the sub-population $S$ in $\mathbb{C}$ that solves:
\begin{equation}
\label{eq: wvio}
    \delta_{m} \equiv \max_{S\in \mathbb{C}}\log\left(\left.\frac{Pr[A=a|S, y]}{Pr[A\neq a|A, y]}\middle/\frac{Pr[A=a|S]}{Pr[A\neq a|S]}\right)\right. .
\end{equation}
To illustrate the challenges of the worst-violation problem, consider the case where there exists $S_{0}, S_{\delta}\in \mathbb{C}$ and a protected group $a\in \mathfrak{A}$ with no violation of multi differential fairness in $S_{0}$ (i.e. $0-$ multi differential fairness and $\delta-$ multi differential fairness violation in $S_{\delta}$ ($\delta >0$) when $Y=1$.  Consider $c, c^{'}\in \mathbb{C}$ such that $c(x)=1$ if and only if $x\in S_{\delta}$ and $c^{'}(x)=1$ if and only if $x\in S_{\delta}\cup S_{0}$. Both $c$ and $c^{'}$ have the same accuracy on $S_{\delta}\cup S_{0}$ if the distribution is balanced. Therefore, algorithm \ref{algo: 1} will pick indifferently $c$ or $c^{'}$ as unfairness certificate, although $c^{'}$ does not single out $S_{\delta}$ as a worst violation. 

\paragraph{Worst Violation Algorithm}
At issue in the previous example is that for sub-population no violation of multi differential fairness, choosing $c=1$ or $c=-1$ will lead to same empirical risk used in \ref{algo: 1}. Algorithm \ref{algo: 3} puts a slightly larger weight on samples $((x_{i}, a_{i}), y_{i})$ whenever $a_{i}\neq a$ so that the empirical risk is now smaller when choosing $c=-1$ whenever there is no violation of multi differential fairness.  More generally, at each iteration, the weight on samples $((x_{i}, a_{i}), y_{i})$ whenever $a_{i}\neq a$ is increased to $1 + \nu t$, with $\nu > 0$ and the solution $c_{t}^{*}$ of the empirical risk minimization \eqref{eq: risk1} or \eqref{eq: risk2} will identify $S\in \mathbb{C}$ as a violation only if $\delta \geq \log(1 + \nu t)$. The algorithm \ref{algo: 3} terminates whenever either $|S|\leq \alpha$ or $c^{*}_{t}(x)=1$ for all samples. At the last iteration before termination, theorem \ref{thm: algo3_ana} shows that algorithm \ref{algo: 3} will identify a sub-population with a $\delta$-multi differential fairness violation such that

\begin{equation}
\delta \geq \delta_{m} - \tilde{O}\left(\frac{1}{\sqrt{m}}\right).
\end{equation}

\begin{thm}
\label{thm: algo3_ana}
Suppose $\nu > 0, \epsilon >0, \eta\in (0, 1)$ and $\mathbb{C}\subset 2^{\mathfrak{X}}$ is $\alpha-$strong. Denote $\delta_{m}$ the worst violation of multi differential fairness for $\mathbb{C}$ as defined in \eqref{eq: wvio}. With probability $1-\eta$, with $O\left(\right)$ samples and after $O\left(\right)$ iterations, algorithm \ref{algo: 3} learns $c\in \mathbb{C}$ such that 
\begin{equation}
    \log\left(\frac{Pr_{w}[A=a|y, c(x)=1]}{Pr_{w}[A\neq a|y, c(x)=1]} \right) \geq \delta_{m} - \epsilon.
\end{equation}
\end{thm}
\begin{algorithm}
\caption{Worst Violation Algorithm}
\label{algo: 3}
\begin{algorithmic}[1]
\State \textbf{Input:}  $\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$, $\mathbb{C}\subset 2^{|\mathfrak{X}|}$, $s$, $tol$
 
 \State $t=0$; $\gamma_{-1}=0$; $\gamma_{0}=1$ 
 
\While {$\abs{\gamma_{t} - \gamma_{t-1}} > tol$}  

\State $st$- noise infusion to transform $\{((x_{i}, a_{i}), y_{i})\}_{i=1}^{m}$ into $\{((x_{i}, a_{i}(st)), y_{i})\}_{i=1}^{m}$ 

\State $c^{*}_{1} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i}(st))\frac{1+y_{i}}{2}\right) $ and $opt_{1}$ 

\State $c^{*}_{2} = argmin_{c\in \mathbb{C}}\frac{1}{m}\displaystyle\sum_{i=1}^{m} \mathbbm{1}\left(\frac{1 +c(x_{i})}{2} \neq \mathbbm{1}_{a}(a_{i}(st))\frac{1-y_{i}}{2}\right) $ and $opt_{2}$
 
 \State $i \gets argmin_{j=1,1}\{opt_{1}, opt_{2}\}$
\State $\hat{\gamma}_{t}\gets \frac{Pr[A=a|c_{i}^{*}=1]Pr[c_{i}^{*}=1, y]}{ w_{c^{*}==1}} $
 
 \State $t\gets t +1$
 
 \EndWhile   
\State{\bfseries Return} $\hat{\gamma}-$ unfair
\end{algorithmic}
\end{algorithm}

\section{Experiments}

\subsection{Synthetic Data}
A synthetic data is constructed by drawing independently two features $X_{1}$ and $X_{2}$ from two normal distribution $N(A\mu, 1)$, where  $\mathfrak{A}=\{-1, 1\}$ is the protected attribute and $\mu \geq 0$ is an unbalance factor. $\mu=0$ means that the data is perfectly balanced. As $\mu$ increases, the distriubtion $Pr(x|A=1)$ becomes more dense in the right uppermost quadrant than the distribution $Pr[x|A=-1]$. The data is labeled according to the sign of $X_{1} + X_{2} + e$, where is $e$ is a noise drawn from $N(0, 0.2)$. The audited classifier $f$ is a logistic regression classifier that is altered to generate instances of metric-free individual unfairness: with probability $1-\nu\in[0, 1)$, the sign of the outcomes from individuals with $A=-1$ is changed from $-1$ to $+1$ if $x^{2}_{1} + x^{2}_{2} \leq 1$ and $x_{1}+ x_{2} \leq 0$; fi $A=1$, all classifier's outcomes are set to $1$ if if $x^{2}_{1} + x^{2}_{2} \leq 1$ and $x_{1}+ x_{2} \leq 0$. For $\nu=0$, the audited classifier is metric-free individual fair; however, as $\nu$ increases, the half circle $\{(x_{1}, x_{2})|x^{2}_{1} + x^{2}_{2} \leq 1 \mbox{ and } \}$ there is a fraction $\nu$ of individuals with protected attribute equal to $1$ who are not treated similarly as individuals with protected value equal to $1$. 

\bigskip
\paragraph{Sample Complexity}
First, the auditing algorithm $AMDF$ is trained using a decision tree and a balanced data ($\mu=0$). Given our setting, for any value of $\nu$ we can compute the actual level of differential unfairness $\gamma_{o}$ and compare it to the value  $\gamma_{a}$ estimated by the auditing algorithm $AMDF$.  Figure \ref{fig: 1a} plots $\gamma_{a}$ against $\gamma_{o}$ after $100$ simulations for value of $\beta$ varying from $0$ to $0.5$ and fixed sub-population size $\alpha=0.1$. The experiment is conducted with data set of size $n\in \{1000, 5000\}$. The estimated unfairness level $\gamma_{a}$ is unbiased since the plots nearly align with $45\degree$ line. Figure \ref{fig: 1b} shows that this result is robust to changing the size $\alpha$ of the sub-population for which there is a fairness violation. Our auditing strategy is able to correctly measure unfairness even when it is limited to a group representing $1\%$ of the overall population. 

\begin{figure}[h!]
\begin{subfigure} {.475\linewidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={$\gamma_{o}$},
    ylabel={$\gamma_{a}$},
    xmin=0, xmax=0.06,
    ymin=0, ymax=0.06,
    xtick={0, 0.02, 0.04, 0.06, 0.08},
    ytick={0, 0.02, 0.04, 0.06, 0.08},
    legend pos=south east,
    ymajorgrids=true,
    grid style=dashed,
    width=\linewidth
]
\addplot[mark=none] table [x=gamma, y=gamma, col sep=comma]
     {../results/synth_exp_sample_size_var.csv};
 \foreach\g in{1000, 5000}{
                \addplot  
                 table [discard if not={size}{\g}, x=gamma, y=estimated_gamma , col sep=comma]
     {../results/synth_exp_sample_size_var.csv};
            }
            \legend{$45\degree$ line, $n=1K$, $n=5K$}

\end{axis}
\end{tikzpicture}
\caption{Effect of unfairness intensity $\beta$ on auditor's performance.}
\label{fig: 1a}
\end{subfigure}
 \hfill%
\begin{subfigure} {.475\linewidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={$\gamma_{o}$},
    ylabel={$\gamma_{a}$},
    xmin=0, xmax=0.04,
    ymin=0, ymax=0.04,
    xtick={0, 0.02, 0.04},
    ytick={0, 0.02, 0.04},
    legend pos=south east,
    ymajorgrids=true,
    grid style=dashed,
    width=\linewidth
]
\addplot[mark=none] table [x=gamma, y=gamma, col sep=comma]
     {../results/synth_exp_sample_size_var.csv};
 \foreach\g in{1000, 5000}{
                \addplot  
                 table [discard if not={size}{\g}, x=gamma, y=estimated_gamma , col sep=comma]
     {../results/synth_exp_sample_size_alpha.csv};
            }
            \legend{$45\degree$ line, $n=1K$, $n=5K$}

\end{axis}
\end{tikzpicture}
\caption{Effect of sub-population size ($\alpha$) on auditor's performance.}
\label{fig: 1b}
\end{subfigure}
\caption{Certifying $\gamma$- multi differential unfairness. The auditor is a decision tree whose depth is tuned using a $5$-fold cross validation; the unbalance parameter $\mu$ is set to $0$.  The plots show the average of $100$ experiments. Auditor's bias when estimating $\gamma_{a}$ is measured by deviations from the $45\degree$ line. Figure \ref{fig: 1a}: the size $\alpha$ of sub-population with a fairness violation is set to $0.1$; and, the intensity $\beta$ of the fairness violation varies from $0$ to $0.5$. Figure \ref{fig: 1b}: the size $\alpha$ of sub-population with a fairness violation varies from $0.01$ to $0.1$; and, the intensity $\beta$ of the fairness violation is set to $0.25$. } 
\end{figure}

\paragraph{Concept Class}
Figure \ref{fig: 2a} runs a similar experiment but varies with decision trees of depth varying from $5$ to $40$. At small level of unfairness, a less complex concept class $\mathbb{C}$ generates less bias in the estimate of $\gamma$. A shorter decision tree out-performs significantly deeper structures. This result, in line with the sample complexity presented in theorem \ref{thm: corr1} justifies the concept of multi fairness: in order to be statistically meaningful, the granularity of the sub-populations for which multi differential fairness is audited for is bounded by the complexity of $\mathbbm{C}$. 


\begin{figure}[h!]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={$\gamma_{o}$},
    ylabel={$\gamma_{a}$},
    xmin=0, xmax=0.06,
    ymin=0, ymax=0.06,
    xtick={0, 0.02, 0.04, 0.06, 0.08},
    ytick={0, 0.02, 0.04, 0.06, 0.08},
    legend pos=south east,
    ymajorgrids=true,
    grid style=dashed,
    width=\linewidth
]
\addplot[mark=none] table [x=gamma, y=gamma, col sep=comma]
     {../results/synth_exp_sample_size_var.csv};
 \foreach\g in{1000, 5000}{
                \addplot  
                 table [discard if not={size}{\g}, x=gamma, y=estimated_gamma , col sep=comma]
     {../results/synth_exp_sample_size_var.csv};
            }
            \legend{$45\degree$ line, $n=1K$, $n=5K$}

\end{axis}
\end{tikzpicture}
\caption{Effect of unfairness intensity $\beta$ on auditor's performance.}
\label{fig: 2a}
\caption{Certifying $\gamma$- multi differential unfairness. The auditor is a decision tree whose depth is tuned using a $5$-fold cross validation; the unbalance parameter $\mu$ is set to $0$.  The plots show the average of $100$ experiments. Auditor's bias when estimating $\gamma_{a}$ is measured by deviations from the $45\degree$ line. Figure \ref{fig: 1a}: the size $\alpha$ of sub-population with a fairness violation is set to $0.1$; and, the intensity $\beta$ of the fairness violation varies from $0$ to $0.5$. Figure \ref{fig: 1b}: the size $\alpha$ of sub-population with a fairness violation varies from $0.01$ to $0.1$; and, the intensity $\beta$ of the fairness violation is set to $0.25$. } 
\end{figure}


\paragraph{Unbalanced Data}
To test the performance of our certifying algorithm on unbalanced data, we repeat the previous experiment with $\mu=0.25$. We compare the performance of four reweighting approaches: uniform weight ($UW$); estimated importance sampling weights ($IPW$); quantile weights ($QW$) with $10$ bins. Figure \ref{fig: 2a} shows that using absent of a reweighting scheme ($UW$) the certifying algorithm fails to estimate correctly the classifier's unfairness: for low level of unfairness ($\gamma_{o} < 2. 10^{-2}$), the bias in $\gamma_{a}$ is over $100\%$. 


\begin{figure}[h!]
\begin{subfigure} {.475\linewidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={$\gamma_{o}$},
    ylabel={$\gamma_{a}$},
    xmin=0, xmax=0.06,
    ymin=0, ymax=0.06,
    xtick={0, 0.02, 0.04, 0.06, 0.08},
    ytick={0, 0.02, 0.04, 0.06, 0.08},
    legend pos=south east,
    ymajorgrids=true,
    grid style=dashed,
    width=\linewidth
]

 \foreach\g in{Uniform, IS, MMD, MMD_NET}{
                \addplot table [discard if not={balancing}{\g}, x=unbalance, y=bias, col sep=comma] {../results/synth_exp_unbalance_3a.csv};
            }
            \legend{$Uniform$,$IS$, $MMD$, $MMD_NET$};
\end{axis}
\end{tikzpicture}
\caption{Varying classifier's unfairness $\gamma_{o}$.}
\label{fig: 2a}
\end{subfigure}
 \hfill%
\begin{subfigure} {.475\linewidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={$\mu$},
    ylabel={$\gamma_{a}/\gamma_{o}$},
    xmin=-0.3, xmax=0.3,
    ymin=0.4, ymax=2.0,
    xtick={-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3},
    ytick={0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
    width=\linewidth
]
  \foreach\g in{Uniform, IPW, IPW_Q, MMD}{
                \addplot table [discard if not={balancing}{\g}, x=unbalance, y=bias, col sep=comma] {../results/synth_exp_unbalance_2b.csv};
            }
            \legend{$Uniform$,$IS$, $IS_Q$, $MMD$};

\end{axis}
\end{tikzpicture}
\caption{Varying data unbalance $\mu$.}
\label{fig: 2b}
\end{subfigure}
\caption{Certifying $\gamma$- multi differential unfairness when data is unbalanced. Figure \ref{fig: 1a}: the auditor is a decision tree with depth $5$; sample size is $10K$; Figure \ref{fig: 1b}: the auditor is a decision tree with depth $5$; sample size is $10K$; $\gamma_{0}=3$. The auditor's bias is measured by the ratio $\gamma_{a}/\gamma_{o}$: a ratio of $1$ means that the auditor measures an unbiased level of unfairness.} 
\end{figure}


\begin{figure}[h!]
\begin{subfigure} {.475\linewidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={$\gamma_{o}$},
    ylabel={$\gamma_{a}/\gamma_{o}$},
    xmin=0, xmax=0.05,
    ymin=0.8, ymax=1.6,
    xtick={0, 0.01, 0.02, 0.03, 0.04, 0.05},
    ytick={ 0.8, 1.0, 1.2, 1.4, 1.6, 1.8},
    legend pos=north east,
    ymajorgrids=true,
    grid style=dashed,
    width=\linewidth
]
 \foreach\g in{0.0001, 0.001, 0.01, 0.1}{
                \addplot table [discard if not={lw}{\g}, x=gamma, y=bias, col sep=comma] {../results/synth_exp_unbalance_3.csv};
            }
            \legend{$\lambda_{w}=10^{-4}$,$\lambda_{w}=10^{-3}$, $\lambda_{w}=10^{-2}$, $\lambda_{w}=10^{-1}$};
\end{axis}
\end{tikzpicture}
\caption{Varying regularization $\lambda_{w}$.}
\label{fig: 3}
\end{subfigure}
 
\caption{Certifying $\gamma$- multi differential unfairness when data is unbalanced. The auditor is a decision tree with depth $5$; sample size is $10K$; The auditor's bias is measured by the ratio $\gamma_{a}/\gamma_{o}$: a ratio of $1$ means that the auditor measures an unbiased level of unfairness.} 
\end{figure}

\section{Appendix}

\subsection{Analysis of Algorithm 2}

Let $a\in\mathfrak{A}$. Without loss of generality we consider the case where $Y=1$. At each iteration $t$, denote $c_{t}^{*}$ the solution of the following optimization problem 
\begin{equation}
\label{eq: opt1}
max_{c\in \mathbb{C}} E_{D_{f}^{w}}\left[\displaystyle\sum_{i=1}^{m} w_{it}\mathbbm{1}_{a}(a_{i})c(x_{i})\middle | Y=1\right],
\end{equation}
where $w_{it}$ are the weights at iteration $t$ and the expectation is taken over all the samples of size $m$ drawn from $D_{f}^{w}$.
\begin{equation}
    w_{it} = \begin{cases}
    w_{i}(1 + \nu t) \mbox{ if } a_{i}\neq a \\
    w_{i} \mbox{ otherwise.}
    \end{cases}
\end{equation}


For $c \in \mathbb{C}$, denote $\beta_{c}=E_{c=1, y=1}[\mathbbm{1}_{a}(a_{i})]$. Let $c_{0}\in \mathbb{C}$ such that $c_{0}(x)=-1$ for all $x\in \mathfrak{X}$. 
Denote $B_{c}=\{x_{i}| g(x_{i})=1 \; f_{a}(x_{i}) =f_{a^{'}}(x_{i})\}$, $B_{g}^{+}=\{x_{i}| g(x_{i})=1 \; f_{a}(x_{i})  >f_{a^{'}}(x_{i})\}$ and $B_{g}^{-}=\{x_{i}| g(x_{i})=1 \; f_{a}(x_{i}) <f_{a^{'}}(x_{i})\}$. Observe that 
\begin{equation}\beta_{g}= \left| \displaystyle\sum_{i, x_{i}\in B_{g}^{+}}w_{i} - \displaystyle\sum_{i, x_{i}\in B_{g}^{-}}w_{i} \right|.
\end{equation}
Assume first that $\displaystyle\sum_{i, x_{i}\in B_{g}^{+}}w_{i} > \displaystyle\sum_{i, x_{i}\in B_{g}^{-}}w_{i}$. Therefore,

\begin{equation}
\begin{split}
   E_{D}\left[\displaystyle\sum_{i=1, g(x_{i})=1}^{m} w_{it}f_{i}^{*}g(x_{i})\right] &=  E_{D}\left[\displaystyle\sum_{i=1, x_{i}\in B_{g} }^{m} w_{it}f_{i}^{*} + \displaystyle\sum_{i=1, x_{i}\notin B_{g} }^{m} w_{it}f_{i}^{*}\right] \\
   & = -\frac{1}{2}\epsilon t E_{D}[|B_{g}|] - 
   \epsilon t  E_{D}[|B_{g}^{-}|] -  E_{D}\left[ \displaystyle\sum_{i=1, x_{i}\in B_{g}^{-} }^{m} w_{i}\right]  \\
   & +  E_{D}\left[ \displaystyle\sum_{i=1, x_{i}\in B_{g}^{+} }^{m} w_{i}\right] \\
   & = -\frac{1}{2}\epsilon t E_{D}[|B_{g}|]- 
   \epsilon t  E_{D}[|B_{g}^{-}|] + \beta_{g},
   \end{split}
\end{equation}
where $|B|=\displaystyle\sum_{i=1, x_{i}\in B}^{m}w_{i}$ for any $B\in \mathbb{C}$. Moreover, 
\begin{equation}
   E_{D}\left[\frac{1}{m}\displaystyle\sum_{i=1, g(x_{i})=1}^{m} w_{it}f_{i}^{*}g_{0}(x_{i})\right] = \frac{1}{2}\epsilon t E_{D}[|B_{g}|] +
   \epsilon t  E_{D}[|B_{g}^{-}|]- \beta_{g}.
\end{equation}
Therefore, $g$ cannot be a solution of \eqref{eq: opt1} if 
\begin{equation}
    \epsilon t\left( E_{D}[|B_{g}|] +
   2  E_{D}[|B_{g}^{-}|]\right) > 2\beta_{g}.
\end{equation}
Note that $|B_{g}| = 1 -\left( |B_{g}^{-}| + |B_{g}^{+}|\right)$ and $\beta_{g}=|B_{g}^{+}| -  |B_{g}^{-}|$. Therefore, $g$ cannot be a solution of \eqref{eq: opt1} if

\begin{equation}
    t > \frac{2\beta_{g}}{\epsilon(1-\beta_{g})}
\end{equation}
At any iteration $t$, a solution of \eqref{eq: opt1} is either $g(x)=-1$ for all $x\in\mathfrak{X}$ or $\beta_{g} > \frac{\epsilon t}{\epsilon t + 2}$. 

\bigskip
\textbf{Small samples properties}
We can use a generic uniform convergence property: 

\begin{thm}
\label{thm1}
Let$\mathfrak{H}$ be a family of function  mapping from $\mathfrak{X}$ to $\{-1, 1\}$ and let $S=\{x_{1}, ..., x_{m}\}$ be a sample where $x_{i}\sim D$ for some distribution $D$ over $\mathfrak{X}$. With probability $1-\delta$, for all $h\in \mathfrak{H}$
$$  \left|E_{S\sim D}[h]- \frac{1}{m}\displaystyle\sum_{i=1}^{m} h(x_{i})\right| \leq 2\mathfrak{R}_{m}(\mathfrak{H}) + \sqrt{\frac{2\ln(1/\delta)}{m}}.$$
\end{thm}

Applying the uniform convergence result from \ref{thm1} allows deriving property of algorithm 2. For any a sample $S$ and any $g\in \mathbb{C}$ with probability $1-\delta/2$, ,
\begin{equation}
    \left|\frac{1}{m}\displaystyle\sum_{i=1, g(x_{i})=1}^{m} w_{it}f_{i}^{*}g(x_{i}) + \frac{1}{2}\epsilon t E_{D}[|B_{g}|] +
   \epsilon t  E_{D}[|B_{g}^{-}|] - \beta_{g}\right| \leq 2\mathfrak{R}_{m}(\mathfrak{\mathbb{C}}) + \sqrt{\frac{2\ln(2/\delta)}{m}},
\end{equation}

and 
\begin{equation}
    \left|\frac{1}{m}\displaystyle\sum_{i=1, g(x_{i})=1}^{m} w_{it}f_{i}^{*}g_{0}(x_{i}) - \frac{1}{2}\epsilon t E_{D}[|B_{g}|] -
   \epsilon t  E_{D}[|B_{g}^{-}|] + \beta_{g}\right| \leq 2\mathfrak{R}_{m}(\mathfrak{\mathbb{C}}) + \sqrt{\frac{2\ln(2/\delta)}{m}}.
\end{equation}
Therefore, with probability $1-\delta$, $h$ cannot be solution of the empirical counterpart of \eqref{eq: opt1} if 
\begin{equation}
     t > \frac{2\beta_{g}}{\epsilon(1-\beta_{g})} + \frac{4}{1-\beta_{g}}\mathfrak{R}_{m}(\mathfrak{\mathbb{C}}) + \frac{2}{1-\beta_{g}}\sqrt{\frac{2\ln(2/\delta)}{m}}.
\end{equation}
Therefore at iteration $t$, with probability $1-\delta$, a solution of \eqref{eq: opt1} for a sample $S$ is either $g(x)=-1$ for all $x\in S$ or 
\begin{equation}
    \beta_{g} > \frac{\epsilon t}{2 + \epsilon t} - \frac{4}{2 + \epsilon t}\mathfrak{R}_{m}(\mathfrak{\mathbb{C}}) - \frac{2}{2 + \epsilon t}\sqrt{\frac{2\ln(2/\delta)}{m}}.
\end{equation}


\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{references}

\end{document}