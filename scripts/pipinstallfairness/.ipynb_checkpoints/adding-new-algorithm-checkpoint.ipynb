{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('notebook')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll suppress warnings because both altair and sklearn are\n",
    "# emitting lots of them, and they're annoying in a demo setting.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import altair as alt\n",
    "# Ask Altair to produce output that works on Jupyter Notebook\n",
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.algorithms.Algorithm import Algorithm\n",
    "import pandas as pd\n",
    "\n",
    "# Simple example that rebalances the training set wrt to the sensitive attribute\n",
    "# to mitigate balancing problems, and uses any baseline SKLearn algorithm for prediction\n",
    "\n",
    "# NB: This isn't a remotely good algorithm! \n",
    "#     But it's very simple to implement and gets the point across.\n",
    "\n",
    "class SensitiveAttrRebalancing(Algorithm):\n",
    "    def __init__(self, algorithm, baseline_name):\n",
    "        # the only field you must initialize here is \"name\", which needs to be a\n",
    "        # unique string that will be used to identify this algorithm in the\n",
    "        # result data frames\n",
    "        Algorithm.__init__(self)\n",
    "        self.classifier = algorithm\n",
    "        self.name = \"SensitiveAttrRebalancing\" + baseline_name\n",
    "        \n",
    "    def run(self, train_df, test_df, class_attr, positive_class_val, sensitive_attrs,\n",
    "            single_sensitive, privileged_vals, params):\n",
    "        # - train_df is a pandas dataframe with the training data\n",
    "        # - test_df is a pandas dataframe with the testing data\n",
    "        # - class_attr is the attribute to be predicted\n",
    "        # - positive_class_value is the assumed \"good\" prediction, which might\n",
    "        #   be necessary information for some methods\n",
    "        # - sensitive_attrs is a list of the columns that are considered sensitive\n",
    "        #   attributes for the sake of fairness\n",
    "        \n",
    "        # - single_sensitive is a the name of the sensitive attribute\n",
    "        #   being handled in this run\n",
    "        # - privileged_vals is a list of values (one per sensitive attribute)\n",
    "        #   considered \"privileged\", which is necessary for one-sided definitions\n",
    "        #   of discrimination\n",
    "        # - params: a description of the hyperparameters being used for the current run\n",
    "        \n",
    "        # for sake of simplicity, we only support datasets with\n",
    "        # a single sensitive attribute        \n",
    "        priv_val = privileged_vals[sensitive_attrs.index(single_sensitive)]\n",
    "        total = len(train_df)\n",
    "        \n",
    "        train_priv     = train_df[train_df[single_sensitive] == priv_val]\n",
    "        train_non_priv = train_df[train_df[single_sensitive] != priv_val]\n",
    "        \n",
    "        train_priv_balance     = train_priv.sample(n=int(total/2), replace=True)\n",
    "        train_non_priv_balance = train_non_priv.sample(n=int(total/2), replace=True)\n",
    "        \n",
    "        train_df = pd.concat([train_priv_balance, train_non_priv_balance])\n",
    "        \n",
    "\n",
    "        # remove sensitive attributes from the training set\n",
    "        train_df_nosensitive = train_df.drop(columns = sensitive_attrs)\n",
    "        test_df_nosensitive = test_df.drop(columns = sensitive_attrs)\n",
    "\n",
    "        # create and train the classifier\n",
    "        classifier = self.classifier()\n",
    "        y = train_df_nosensitive[class_attr]\n",
    "        X = train_df_nosensitive.drop(columns = class_attr)\n",
    "        classifier.fit(X, y)\n",
    "\n",
    "        # get the predictions on the test set\n",
    "        X_test = test_df_nosensitive.drop(class_attr, axis=1)\n",
    "        predictions = classifier.predict(X_test)\n",
    "        print(predictions)\n",
    "\n",
    "        return predictions, []\n",
    "\n",
    "    def get_supported_data_types(self):\n",
    "        # There are currently four types of possible data types:\n",
    "        #  - numerical\n",
    "        #  - numerical-binsensitive\n",
    "        #  - categorical\n",
    "        #  - categorical-binsensitive\n",
    "        #\n",
    "        # a \"numerical\" datatype is one in which every column is numerical. If your algorithm\n",
    "        # only supports numerical values (like an SVM or LR), then use \"numerical-*\" options\n",
    "        #\n",
    "        # a \"-binsensitive\" suffix refers to the fact that the sensitive attribute must be binary.\n",
    "        # If your algorithm supports more than binary sensitive attributes, then you can\n",
    "        # return \"categorical\" or \"numerical\" in this method.\n",
    "        #\n",
    "        # For the sake of simplicity, we use only the simplest option, \"numerical-binsensitive\".\n",
    "        # In this case, the `fairness` package binarizes the sensitive attribute (but do note this\n",
    "        # introduces potential problems with intersectionality, see Buolamwini and Gebru's 2018 FAT*\n",
    "        # paper for a clear example).\n",
    "        return set([\"numerical-binsensitive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available algorithms:\n",
      "  SVM\n",
      "  GaussianNB\n",
      "  LR\n",
      "  DecisionTree\n",
      "  Kamishima\n",
      "  Calders\n",
      "  ZafarBaseline\n",
      "  ZafarFairness\n",
      "  ZafarAccuracy\n",
      "  Kamishima-accuracy\n",
      "  Kamishima-DIavgall\n",
      "  Feldman-SVM\n",
      "  Feldman-GaussianNB\n",
      "  Feldman-LR\n",
      "  Feldman-DecisionTree\n",
      "  Feldman-SVM-DIavgall\n",
      "  Feldman-SVM-accuracy\n",
      "  Feldman-GaussianNB-DIavgall\n",
      "  Feldman-GaussianNB-accuracy\n"
     ]
    }
   ],
   "source": [
    "import fairness\n",
    "import fairness.benchmark\n",
    "from sklearn.tree import DecisionTreeClassifier as SKLearn_DT\n",
    "\n",
    "fairness.add_algorithm(SensitiveAttrRebalancing(SKLearn_DT, \"DecisionTree\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: '['heart-disease_numerical-binsensitive']'\n"
     ]
    }
   ],
   "source": [
    "fairness.benchmark.run(algorithm=[\"SensitiveAttrRebalancingDecisionTree\"], dataset=[\"heart-disease_numerical-binsensitive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ricci_Race = fairness.get_dataset_by_name(\"ricci\").get_results_data_frame(\"Race\", \"numerical-binsensitive\")\n",
    "ricci_Race = ricci_Race[ricci_Race.algorithm.str.contains(\"DecisionTree\")]\n",
    "alt.Chart(ricci_Race).mark_point().encode(\n",
    "    x='accuracy',\n",
    "    y='DIbinary',\n",
    "    color='algorithm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
